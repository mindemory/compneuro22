{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have a network of three neurons with activity (firing rate) given by $x_1 $, $x_2 $, $x_3 $, respectively. The neurons are arranged in a sequence and the firing rate of neuron 3 is given by:\n",
    "#### $x_3 = w_2 x_2 = w_2 w_1 x_1 $\n",
    "#### where, $w_1 $ and $w_2 $ are the synaptic weights connecting neuron 1 to neuron 2 and neuron 2 to neuron 3, respectively.\n",
    "#### The goal is to train this network such that the firing rate of neuron 3 matches a target firing rate $y $. In order to do so, we use a quadratic loss function given by:\n",
    "#### $l(y, x_3) = \\frac{1}{2}(y-x_3)^2 $\n",
    "#### Our goal is to determine the set of weights for which the loss function is minimized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Warm-up. If $x_1 > 0 $, $w_1 < 0 $, and $x_3 < y $, should $w_2 $ increase or decrease to improve the loss? Should $w_1 $ increase or decrease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since $w_1 < 0 $ and $x_1 > 0 $, $w_2 $ should decrease and $w_1 $ should decrease for the loss to improve. This is because when $w_1 > 0 $, a negative $w_1 $ will cause $x_2 $ and $x_3 $ to have a negative activity. Since $x_3 < y $, this means that the activity of $x_3 $ is too low, so increasing $w_2 $ (which will increase the activity of $x_3 $) and decreasing $w_1 $ (which will decrease the activity of $x_3 $) will help to reduce the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be more precise than the above procedure, we use gradient descent, which computes derivatives of the loss function with respect to the synaptic weights, and then uses these derivatives to iteratively update the weights. Backpropagation is simply a procedure for computing these derivatives by the chain rule taught in any standard calculus course.\n",
    "## b) Compute the derivative of $l $ with respect to $x_3 $. Then, using the chain rule compute the derivatives with respect to $w_2 $, $x_2 $ and $w_1 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The loss function is given by:\n",
    "#### $l(y, x_3) = \\frac{1}{2}(y-x_3)^2 $\n",
    "#### Computing the derivative of $l $ with respect to $x_3 $:\n",
    "#### $\\frac{dl}{dx_3}  = \\frac{d}{dx_3} \\frac{1}{2} (y-x_3)^2 $\n",
    "#### $\\therefore \\frac{dl}{dx_3}  = \\frac{1}{2} \\times 2 (y-x_3) \\times (-1) $\n",
    "#### $\\therefore \\frac{dl}{dx_3}  = x_3 - y $\n",
    "#### $\\because x_3 = w_2 x_2 $, we can rewrite the partial derivate wrt $w_2 $ as:\n",
    "#### $\\frac{\\partial l}{\\partial w_2} = \\frac{\\partial l}{\\partial x_3} \\times \\frac{\\partial x_3}{\\partial w_2} $\n",
    "#### $\\therefore \\frac{\\partial l}{\\partial w_2} = (x_3 - y) \\times \\frac{\\partial w_2 x_2}{\\partial w_2} $\n",
    "#### $\\therefore \\frac{\\partial l}{\\partial w_2} = (x_3 - y) x_2$\n",
    "#### Similarly, the partial derivative wrt $x_2 $ can be written as: \n",
    "#### $\\frac{\\partial l}{\\partial x_2} = \\frac{\\partial l}{\\partial x_3} \\times \\frac{\\partial x_3}{\\partial x_2} $\n",
    "#### $\\therefore \\frac{\\partial l}{\\partial x_2} = (x_3 - y) \\times \\frac{\\partial w_2 x_2}{\\partial x_2} $\n",
    "#### $\\therefore \\frac{\\partial l}{\\partial x_2} = (x_3 - y) w_2$\n",
    "#### We also have: $x_3 = w_2 w_1 x_1 $, therefore we can rewrite the partial derivative wrt $w_1 $ as:\n",
    "#### $\\frac{\\partial l}{\\partial w_1} = \\frac{\\partial l}{\\partial x_3} \\times \\frac{\\partial x_3}{\\partial w_1} $\n",
    "#### $\\therefore \\frac{\\partial l}{\\partial w_1} = (x_3 - y) \\times \\frac{\\partial w_2 w_1 x_1}{\\partial w_1} $\n",
    "#### $\\therefore \\frac{\\partial l}{\\partial w_1} = (x_3 - y) w_2 x_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Using your results above, write down an update rule that is guaranteed to decrease the loss function for a sufficiently small step size (learning rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The update rule that is guaranteed to decrease the loss function for a sufficiently small step size (learning rate) is:\n",
    "#### $w_2 = w_2 - \\alpha (x_3 - y) x_2 $\n",
    "#### $w_1 = w_1 - \\alpha (x_3 - y) w_2 x_1 $\n",
    "#### where, $\\alpha $ is the learning rate.\n",
    "#### This update rule uses the derivatives that we computed earlier to adjust the weights in a way that will reduce the loss. The learning rate determines the size of the update, and if it is sufficiently small then the updated weights will always result in a lower loss than the previous weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward backpropagation (nonlinear case)\n",
    "## Real biological networks are nonlinear, as are artificial networks used in practice in machine learning. Consider a slight modification to the above network where $x_2 $ has a nonlinear activation function (usually, the final layer of the network is linearly read out in machine learning applications). Specifically, consider the model:\n",
    "## $x_3 = w_2 \\sigma (w_1 x_1) $\n",
    "## where, $\\sigma $ is a nonlinear function. Unless otherwise specified, simply assume that $\\sigma $ is an arbitrary monotonically increasing nonlinear function with derivative $\\sigma^{'} $\n",
    "## d) Recompute your answers to question (b) with the nonlinearity. How do your answers change after the introduction of this nonlinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The derivative of $l $ with respect to $x_3 $ is still:\n",
    "#### $ \\frac{dl}{dx_3}  = (x_3 - y) $\n",
    "#### However, since $x_3 $ is now a nonlinear function of $x_2 $ and $x_1 $, the derivatives with respect to $w_2 $, $x_2 $ and $w_1 $ will change. We can use the chain rule to compute these derivatives as follows:\n",
    "#### $\\frac{\\partial l}{\\partial w_2} = \\frac{\\partial l}{\\partial x_3} \\times \\frac{\\partial x_3}{\\partial w_2} $\n",
    "#### $\\therefore \\frac{\\partial l}{\\partial w_2} = (x_3 - y) \\times \\sigma (w_1 x_1) $\n",
    "#### $\\frac{\\partial l}{\\partial x_2} = \\frac{\\partial l}{\\partial x_3} \\times \\frac{\\partial x_3}{\\partial x_2} $\n",
    "#### $\\therefore \\frac{\\partial l}{\\partial x_2} = (x_3 - y) \\times w_2  \\times \\sigma^{'} (w_1 x_1) \\times w_1$\n",
    "#### $\\frac{\\partial l}{\\partial w_1} = \\frac{\\partial l}{\\partial x_3} \\times \\frac{\\partial x_3}{\\partial w_1} $\n",
    "#### $\\therefore \\frac{\\partial l}{\\partial w_1} = (x_3 - y) \\times w_2  \\times \\sigma^{'} (w_1 x_1) \\times x_1$\n",
    "#### These derivatives are different from the ones computed in the linear case because the nonlinearity introduces additional terms that need to be accounted for.\n",
    "#### The update rule that is guaranteed to decrease the loss function for a sufficiently small step size (learning rate) is:\n",
    "#### $w_2 = w_2 - \\alpha (x_3 - y) \\times \\sigma (w_1 x_1) $\n",
    "#### $w_1 = w_1 - \\alpha (x_3 - y) \\times w_2 \\times \\sigma^{'} (w_1 x_1) \\times x_1 $\n",
    "#### This update rule is similar to the one in the linear case, but it incorporates the nonlinearity by using the derivatives that we computed above. Again, the learning rate determines the size of the update, and if it is sufficiently small then the updated weights will always result in a lower loss than the previous weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A typical choice for $\\sigma $ in machine learning applications is the rectified linear unit (ReLU), $ \\sigma (x) = max(0, x) $. Compute the gradient of the weights for this choice of $\\sigma $. Can you foresee any problems for the backpropagation learning rule in this scenario? (Hint: think about what happens when $x_1 $ and $w_1 $ have opposite signs.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we choose the ReLU as the nonlinearity, then the gradient of the weights is given by:\n",
    "#### $\\frac{\\partial l}{\\partial w_2} = \\frac{\\partial l}{\\partial x_3} \\times \\frac{\\partial x_3}{\\partial w_2} $\n",
    "#### $\\therefore \\frac{\\partial l}{\\partial w_2} = (x_3 - y) \\times max(0, w_1 x_1) $\n",
    "#### $\\frac{\\partial l}{\\partial x_2} = \\frac{\\partial l}{\\partial x_3} \\times \\frac{\\partial x_3}{\\partial x_2} $\n",
    "#### $\\therefore \\frac{\\partial l}{\\partial x_2} = (x_3 - y) \\times w_2  \\times f(w_1 x_1) \\times w_1$\n",
    "#### $\\frac{\\partial l}{\\partial w_1} = \\frac{\\partial l}{\\partial x_3} \\times \\frac{\\partial x_3}{\\partial w_1} $\n",
    "#### $\\therefore \\frac{\\partial l}{\\partial w_1} = (x_3 - y) \\times w_2  \\times f(w_1 x_1) \\times w_1$\n",
    "#### where, $f(w_1 x_1) = 1 $, if $w_1 x_1 > 0 $ and is 0 otherwise.\n",
    "#### One potential problem with this backpropagation learning rule is that if $x_1 $ and $w_1 $ have opposite signs, then the gradient $w_1 $ will be 0 because the ReLU will always be 0 in this case. This means that the weight $w_1 $ will not be updated, which can slow down or even prevent the learning process. One way to address this problem is to use a different nonlinearity that does not have this issues, such as the sigmoid function or the hyperbolic tangent. These nonlinearities have a non-zero gradient for all input values, which allows the weights to be updated even when $x_1 $ and $w_1 $ have opposite signs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6 (default, Sep 19 2022, 18:46:30) \n[Clang 14.0.0 (clang-1400.0.29.201)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
